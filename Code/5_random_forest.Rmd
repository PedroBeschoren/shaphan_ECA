---
title: "5_random_fortest"
author: "Pedro"
date: "8/9/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

# 0 - Load libraries & data

```{r}

#load necessary libraries
library(phyloseq)
library(ggplot2)
library(tibble)
library(dplyr)
library(Boruta)
library(caret)
library(randomForest)
library(metacoder)

# increases memory limit used by R
memory.limit(size = 350000)

#load the phyloseq objects containing all microbiome data
load(file = "./Data/CSS_BacFun_ps_l.RData")


```

# 1 - define a few functions of interest
```{r}

#prepare input to boruta
single_physeq_to_borutaInput<-function (physeq_object, variable_to_be_classified){
  # boruta expects a transposed OTU table with variable_to_be_classified as a added variable
  # the output is a list of df ready to be used as input to boruta  
  #transpose phtseq otu table  
  otu_cells_wide_list <- #transpose the feature table...
    base::as.data.frame(t(otu_table(physeq_object)))%>%
      rownames_to_column(var = "sample")
  
  # extract sample data
  metadata_list <-
    as(sample_data(physeq_object),"data.frame")%>%
      rownames_to_column(var = "sample")
  
  #add the variable classification you want to predict with the random forest
  boruta_dataset<-
    base::merge(dplyr::select(metadata_list,sample, variable_to_be_classified),
                otu_cells_wide_list,
                by = "sample",
                all.y = TRUE)
  
  #make sure your variable to be classified is a numeic, or boruta won't run
  
    boruta_dataset[,2]<-as.numeric(boruta_dataset[,2]) # saves the second column, your variable_to_be_classified, as numeric
   
    output<-boruta_dataset
    
  gc()
  return(output)
}

# write a function to split training and test data from a phyloseq object
train_and_test_spliter<-function(ps_object, variable_to_be_classified){
  
  # this function will separate train and test sets based on a phyloseq object and the variable to be predicted. it requires the function single_physeq_to_borutaInput()
  # ps_object = a phyloseq object
  # variable_to_be_classified =  a (quoted) metadata column that you want to predict
  # the output is a list of two objects: the first is the training set, the second is the test set
  
  # wrangle phyloseq data
  ps_data<-single_physeq_to_borutaInput(physeq_object = ps_object,
                                        variable_to_be_classified = variable_to_be_classified)
  
  # define training and test set. this can be ofptimized for repeated k-fold cross validation
  trainIndex<- createDataPartition(ps_data[,2], 
                                   p = .70, 
                                   list = FALSE, 
                                   times = 1)
  # set train and test sets
  data_Train <- ps_data [ trainIndex,]
  data_Test  <- ps_data [-trainIndex,]
  
  output<-list(data_Train,data_Test)
  names(output)<-c("data_Train","data_Test")
  
  return(output)
  
}

# define a function to fix borta objects and put them into formula format
fixed_boruta_formula<-function(boruta_object){
  # this fucntion takes a boruta ofbect, fixes the inconclusive tas into importnat o unimportnat, and then generates a formula
  # the input is a boruta object
  # the output is a boruta formula to be fed to caret::train
  # NOTE: boruta objects with zero imporntat features may crash!
  
  fixed_boruta<-TentativeRoughFix(boruta_object)
  boruta_imp_ASV<-getSelectedAttributes(fixed_boruta)
  print("number of importnat ASVs. Warning: if zero, formula will crash!")
  print(length(boruta_imp_ASV)%>%unlist()%>%sort())
  formula_boruta<-getConfirmedFormula(fixed_boruta)
  
  return(formula_boruta)
}

# put fixing, spliting, training and testing all in a single function
fix_split_train<-function (boruta_output_l, ps_object_l, variable_to_be_classified){
  # this fucntion will fix tentative features in a list of boruta objects
  # then it will split a list of phyloseq objects into training and test sets
  # then it will train the list of models
  # then it will test the lsit of models
  # then it returns a list of confusion matrixes (one for each model)
    # boruta_output_l = a list of boruta objects
    # ps_object_l = a list of phyloseq objects
    # variable_to_be_classified = the metadata varaible you are trying to predict (must be quoted, like "Stress")
  
  # fix boruta in a formula to be evaluated with caret
  boruta_formula_bac_l<-lapply(boruta_output_l, function(x) fixed_boruta_formula(x))
  
  # split train adn test dataset
  train_test_l<-lapply(ps_object_l, function (x)
    train_and_test_spliter(ps_object = x, 
                           variable_to_be_classified = variable_to_be_classified))
  
  
  
  # train model
  boruta_feature_rf_repeatedcv<-mapply(function (x,z) {
    
    train.control <- caret::trainControl(method = "repeatedcv", # set trainig/data split controls for the train function
                                         number = 5,
                                         repeats = 10,
                                         allowParallel = TRUE)
    
    model_borutized <- caret::train(form = z, # bruta formula
                                    data = x[[1]], # training data ; first element of train_and_test_spliter()
                                    method = "rf", #execute training based on RF
                                    trControl = train.control, # defined in trainControl() above
                                    ntree=600,
                                    tuneLength = 5  )
    
    
    
    return(model_borutized)
  },
  x = train_test_l,
  z = boruta_formula_bac_l,
  SIMPLIFY = FALSE)
  
  
  output<-list("train_test_l" = train_test_l,
               "boruta_feature_rf_repeatedcv" = boruta_feature_rf_repeatedcv)
  
  
  return(output)
  
  
}

# define function to extract some key model metrics after CV precision testing
extract_confusionmatrix_metrics<-function(CV_output_l, Imp_ASV_stats_l){
  # this fucntion will extract key RF model metris from lists of RF models
  # CV_output_l = list of model testing objects from fix_split_train_test() custom function
  # Imp_ASV_stats_l = list of important ASV stats from a fixed boruta object
  # the output is a df with metrics for each model

  # accurayc, kappa, AccuracyPValue  
  
  cv_metrics<-map(CV_output_l,3)
  
  accuracy<-unlist(map(cv_metrics,1)) #accuracy
  
  kappa<-unlist(map(cv_metrics,2)) #kappa
  AccuracyLower<-unlist(map(cv_metrics,3)) #AccuracyLower  
  AccuracyUpper<-unlist(map(cv_metrics,4)) #AccuracyUpper  
  AccuracyPValue<-unlist(map(cv_metrics,6)) #AccuracyPValue 
  
  
  
  model_metrics<-as.data.frame(accuracy)
  model_metrics$kappa<-kappa     
  model_metrics$AccuracyLower<-AccuracyLower  
  model_metrics$AccuracyUpper<-AccuracyUpper  
  model_metrics$AccuracyPValue<-AccuracyPValue
  
  # n important stress predictor ASVs  
  model_metrics$n_imp_ASVs<-lapply(Imp_ASV_stats_l, function (x) length(x[,1]))%>%unlist
  
  # mean average imporance of ASVs in model
  model_metrics$mean_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,2]))%>%unlist
  
  # mean median imporance of ASVs in model
  model_metrics$median_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,3]))%>%unlist
  
  # sd imporance of ASVs in model
  model_metrics$sd_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) sd(x[,2]))%>%unlist
  
  # men normHits of ASVs in model
  model_metrics$mean_normHits_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,6]))%>%unlist
  
  # sd normHits of ASVs in model
  model_metrics$sd_normHits_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) sd(x[,6]))%>%unlist
  
  return(model_metrics)
}


```


# 3 - remove frass-only samples, adjsut column names, fix list
```{r}

#change name of the emtdata column "sample" to avoid crashing with custom function
CSS_BacFun_ps_l<-lapply(CSS_BacFun_ps_l, function(x){
colnames(x@sam_data)[8]<-"sample_type"
return(x)
})

# split bacterial dataset
CSS_BacFun_BulkRhizo_ps_l_l<-lapply( CSS_BacFun_ps_l, function (x) metagMisc::phyloseq_sep_variable(x, variable = "sample_type"))

# remove frass sample type from list, then flatten it
CSS_BacFunBulkRhizo_ps_l<-list("Bac_Bulk" = CSS_BacFun_BulkRhizo_ps_l_l$Bac$bulk_soil,
                                        "Bac_Rhizo" = CSS_BacFun_BulkRhizo_ps_l_l$Bac$rhizosphere,
                                        "Fun_Bulk" = CSS_BacFun_BulkRhizo_ps_l_l$Fun$bulk_soil,
                                        "Fun_Rhizo" = CSS_BacFun_BulkRhizo_ps_l_l$Fun$rhizosphere)

```

# 2 - prepare phyloseq object to be an input in Boruta

check this tutorial for regression on caret: http://zevross.com/blog/2017/09/19/predictive-modeling-and-machine-learning-in-r-with-the-caret-package/
https://towardsdatascience.com/a-guide-to-using-caret-in-r-71dec0bda208
https://rpubs.com/cliex159/881990

start with fun_rhizo (including samples N), then try list





# run tests above on lists... hope it won't crash!

i've silenced the saving function so it does not overwrite the working .RData object with boruta models

running this chunk on my computer (4 cores, 16GB) takes ~ 30 min for fungi dataset and ~ 7h for the abcterial dataset
```{r}

t0<-Sys.time()
set.seed(101)
Boruta_Fun_Biomass<-lapply(CSS_BacFunBulkRhizo_ps_l[3:4], function (x) #parLapply wil do a parallel lapply on the defined cluster
  Boruta(shoot_biomass~.,   # classification you are trying to predict
         data = single_physeq_to_borutaInput(physeq_object = x,
                                             variable_to_be_classified = "shoot_biomass")[,-1], # removes first column, "sample" as a predictor variable
         doTrace=2, 
         maxRuns = 300,  #increase the maximum number of runs to decrease the number of tenttively important OTUs.
         ntree = 3000))

#save
#save(Boruta_Fun_Biomass, file = "./Data/Boruta_Fun_Biomass.RData")


# clear space
rm(Boruta_Fun_Biomass)
gc()


#run bac
Boruta_Bac_Biomass<-lapply(CSS_BacFunBulkRhizo_ps_l[1:2], function (x) #parLapply wil do a parallel lapply on the defined cluster
  Boruta(shoot_biomass~.,   # classification you are trying to predict
         data = single_physeq_to_borutaInput(physeq_object = x,
                                             variable_to_be_classified = "shoot_biomass")[,-1], # removes first column, "sample" as a predictor variable
         doTrace=2, 
         maxRuns = 300,  #increase the maximum number of runs to decrease the number of tenttively important OTUs.
         ntree = 3000))

#save
#save(Boruta_Bac_Biomass, file = "./Data/Boruta_Bac_Biomass.RData")
t1<-Sys.time()


```



```{r}
# load boruta objects
load(file = "./Data/Boruta_Bac_Biomass.RData")
load(file = "./Data/Boruta_Fun_Biomass.RData")

# put output into a single list with four elements
Boruta_Bac_Biomass
Boruta_Fun_Biomass

Boruta_BacFun_Biomass<-list("Bac_Bulk" = Boruta_Bac_Biomass$Bac_Bulk,
                            "Bac_Rhizo" = Boruta_Bac_Biomass$Bac_Rhizo,
                            "Fun_Bulk" = Boruta_Fun_Biomass$Fun_Bulk,
                            "Fun_Rhizo" = Boruta_Fun_Biomass$Fun_Rhizo)

```



# fix split and train in a single replicated function
~ 4h to run; file too big for github
```{r}
set.seed(23456)
replicated_model<-replicate(10, fix_split_train(boruta_output_l = Boruta_BacFun_Biomass,
                                                ps_object_l = CSS_BacFunBulkRhizo_ps_l,
                                                variable_to_be_classified ="shoot_biomass" ))

# save(replicated_model, file = "./Data/replicated_model.RData")
load( file = "./Data/replicated_model.RData")

```




# test replicated models
## quick look in some results
```{r}

# shows RMSE  and Rsquared  for each mtry
lapply(replicated_model[2,], function(y)
  lapply(y, function(x) # in each fo the 10 models..
  arrange(x$results, RMSE) )) # check the results of each 4-split


################ test models on test set
# i want average+-sd R2 and RMSE from the 10 models

#quick plot of predicted VS test set
library(ggpubr)
ggplot(
  data = replicated_model[1,1]$train_test_l$Bac_Bulk$data_Test,
  aes(x = replicated_model[1,1]$train_test_l$Bac_Bulk$data_Test$shoot_biomass,
      y = caret::predict.train(object = replicated_model[2,1]$boruta_feature_rf_repeatedcv$Bac_Bulk,
                                 newdata = replicated_model[1,1]$train_test_l$Bac_Bulk$data_Test))) + 
  geom_point()+
   stat_cor(aes(label = paste(after_stat(rr.label), after_stat(p.label), sep = "~`,`~")))+
  geom_smooth(method = "lm")



replicated_model[2,1]$boruta_feature_rf_repeatedcv$Bac_Bulk$finalModel$forest$bestvar
  
```

# calculate R2 and RSME agaisnt test set
```{r}
# loop to calculate R2 and RSME across a replicatd list of test.train data and their respective models

#### loop starts ###

#create empty list to store results
R2_RMSE_l<-list() 

# define range of data partitions like Bac_bulk
for (i in 1:length(Boruta_BacFun_Biomass)){ 

  # save each iterations of the 10 replicated models....
  R2_RMSE_l[[i]]<-mapply(function(x,y){
  
    # predict biomas values in test set
    prediction<-caret::predict.train(object = y[[i]],
                                     newdata = x[[i]]$data_Test)
    
    # define the eror between test and training set
    error<- prediction - x[[i]]$data_Test$shoot_biomass
    
    
    #calculate r2
    r2_output<-cor(x[[i]]$data_Test$shoot_biomass, prediction)^2
    
    #calculate Root Mean Square Error
    RMSE_output<-sqrt(mean(error ^ 2))
    
    #combine metrics and set column names
    output<-c(r2_output,RMSE_output)
    names(output)<-c("R2", "RMSE")
    
    return(output)

    },
    x = t(replicated_model)[,1], # train/test, 10x4 splits
    y = t(replicated_model)[,2], # models, 10x4 splits
    SIMPLIFY = FALSE)
}

#### loop ends ###

# adjsut names of data partitions
names(R2_RMSE_l)<-names(Boruta_BacFun_Biomass)

# put metrics from the 10 models into a list of data frames
R2_RMSE_l<-lapply(R2_RMSE_l, function(x)
  as.data.frame(do.call(rbind, x)))

#put data paritions as avariable, remove unecessary characthers
R2_RMSE_df<-do.call(rbind, R2_RMSE_l)%>%rownames_to_column(var = "data_parition")
R2_RMSE_df$data_parition<-gsub(pattern = "\\..*", replacement = "", x = R2_RMSE_df$data_parition)

#seaprate data partitions into two columsn
R2_RMSE_df<-tidyr::separate(data = R2_RMSE_df,
                            col = data_parition,
                            into = c("Kingdom", "Compartment"),
                            sep = "_")
```


# plot model R2 and RMSE agasint test set
```{r}
#plot R2
r2_RF<-ggplot(R2_RMSE_df, aes(x = Kingdom, y = R2, fill = Compartment ))+
  geom_violin(alpha = 0.5, position=position_dodge(1))+
  geom_dotplot(binaxis = "y",
               stackdir = "center",
              position=position_dodge(1),
               dotsize = 0.5) +
  theme_bw()+
  ggtitle(label = "R2 of 10 RF models",
           subtitle = "based on ASVs set as important by Boruta" )


#plot RMSE
RMSE_RF<-ggplot(R2_RMSE_df, aes(x = Kingdom, y = RMSE, fill = Compartment ))+
  geom_violin(alpha = 0.5, position=position_dodge(1))+
  geom_dotplot(binaxis = "y",
               stackdir = "center",
              position=position_dodge(1),
               dotsize = 0.5) +
  theme_bw()+
  ggtitle(label = "Root Mean Square error of 10 RF models",
           subtitle = "based on ASVs set as important by Boruta" )


# save both plots as pdf
ggsave(filename = "./Results/RF_10models_R2_RMSE_IncludesNoFrass.pdf",
       plot = ggarrange(r2_RF,RMSE_RF, common.legend = TRUE, labels ="AUTO"),
       width = 30,
       height = 30,
       units = "cm")



```


#check taxonomies of Boruta-selected features
```{r}
# fix oruta
fixed_boruta<-lapply(Boruta_BacFun_Biomass, function(x)
  TentativeRoughFix(x))

# get ocnfirmed ASVs
ASV_stats_confirmed<-lapply(fixed_boruta, function(x)
  filter(attStats(x), decision == "Confirmed"))

#get ASV ID
ASV_confirmed_l<-lapply(ASV_stats_confirmed, function(x) rownames(x))


```



# explote taxonomy and importance of features
```{r}

imp_ps<-prune_taxa(taxa = ASV_confirmed_l[[1]], x = CSS_BacFunBulkRhizo_ps_l$Bac_Bulk) 

imp_melted<-psmelt(imp_ps)

to_merge<-rownames_to_column(ASV_stats_confirmed$Bac_Bulk[,1:2], var = "OTU")

imp_melted<-left_join(imp_melted, to_merge, by ="OTU")

imp_melted%>%
  group_by(Phylum)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(CSS_BacFunBulkRhizo_ps_l$Bac_Bulk),
            linear_biomass_corr = cor(shoot_biomass, Abundance))%>%view

imp_melted%>%
  group_by(Class)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/177,
            linear_biomass_corr = cor(shoot_biomass, Abundance))%>%view

imp_melted%>%
  group_by(Order)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/177,
            linear_biomass_corr = cor(shoot_biomass, Abundance))%>%view

           
imp_melted%>%
  group_by(Family)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/177,
            linear_biomass_corr = cor(shoot_biomass, Abundance))%>%view


imp_melted%>%
  group_by(Family)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/177,
            linear_biomass_corr = cor(shoot_biomass, Abundance))%>%dplyr::arrange(desc(n))




```




# explote taxonomy and importance of features
```{r}
# define a function to extract  mean importance and linear biomass correlations of each taxonomy of the ASVs selected by the random forest
Importance_per_taxon<-function(ps, ASV_stats_boruta){
  
  confirmed_ASV<-rownames(ASV_stats_boruta)
  
imp_ps<-prune_taxa(taxa = confirmed_ASV, x = ps) 

imp_melted<-psmelt(imp_ps)

to_merge<-rownames_to_column(ASV_stats_boruta[,1:2], var = "OTU")

imp_melted<-left_join(imp_melted, to_merge, by ="OTU")

phylum_table<-imp_melted%>%
  group_by(Phylum)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps),
            linear_biomass_corr = round(cor(shoot_biomass, Abundance), digits = 3))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

class_table<-imp_melted%>%
  group_by(Class)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps),
            linear_biomass_corr = round(cor(shoot_biomass, Abundance), digits = 3))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

order_table<-imp_melted%>%
  group_by(Order)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps),
            linear_biomass_corr = round(cor(shoot_biomass, Abundance), digits = 3))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

           
family_table<-imp_melted%>%
  group_by(Family)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps),
            linear_biomass_corr = round(cor(shoot_biomass, Abundance), digits = 3))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

genus_table<-imp_melted%>%
  group_by(Genus)%>%
  summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps),
            linear_biomass_corr = round(cor(shoot_biomass, Abundance), digits = 3))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

output<-list("phylum_table" = phylum_table,
             "class_table" = class_table,
             "order_table" = order_table,
             "family_table" = family_table,
             "genus_table" = genus_table)
return(output)

}

# execute custom function on list of boruta and ps obejcts
importance_per_taxonomy<-mapply(function(x,y)
  Importance_per_taxon(ps = x,ASV_stats_boruta = y),
  x = CSS_BacFunBulkRhizo_ps_l,
  y = ASV_stats_confirmed,
  SIMPLIFY = FALSE)


#export this table as txt
capture.output(importance_per_taxonomy, file = "./Results/importance_per_taxonomy.txt")

```







# heat tree 
```{r}
#prepare ps object for heat tree
CSS_BacFunBulkRhizo_ps_l<-lapply(CSS_BacFunBulkRhizo_ps_l, function(x){
  
  # let's remove the "r__"ranks from the taxonomy, they can be useful but will pollute our plot
tax_table(x)[, colnames(tax_table(x))] <- gsub(pattern = "[a-z]__", # regular expression pattern to search for
                                                                   x = tax_table(x)[, colnames(tax_table(x))], # "df"
                                                                   replacement = "") # replacement for pattern
x@tax_table<-x@tax_table[,1:6]
return(x)
})




```



## 8 -  Visualize the flower features on a heat tree 

```{r}
#deine a function to put the ASV importance from boruta into a heat tree, using a phyloseq object
ps_and_boruta_to_heat_tree<-function(ps_obj, boruta_ASV_stats){

####################### now make it a metacoder object

#rename boruta importances
rf_importance_byOTU<-boruta_ASV_stats

# transform from phyloseq to  taxmap object
rf_important_metacoder<-parse_phyloseq(prune_taxa(taxa = rownames(boruta_ASV_stats), x = ps_obj) )



#get abundance per taxon
rf_important_metacoder$data$tax_abund<-calc_taxon_abund(obj = rf_important_metacoder, 
                                      data = "otu_table",
                                      cols = rf_important_metacoder$data$sample_data$sample_id)


########### let's add RF importance to the metacoder object######

# add metacoder taxon ID to RF importnat OTUs
rf_importance_byOTU$taxon_id<-rf_important_metacoder$data$otu_table$taxon_id

# add RF importance to metacoder object
rf_important_metacoder$data$RF_importance<-tibble(rf_importance_byOTU[,c(1:2,7)])

#get summed importance per taxon
rf_important_metacoder$data$RF_importance_nodes<-calc_taxon_abund(obj = rf_important_metacoder, 
                                      data = "RF_importance")

#define number of subtaxa
subtaxa_number<-n_subtaxa(rf_important_metacoder)+1

# transform zeros into 1 so you don't try to divide by zero on the next step
subtaxa_number[subtaxa_number==0]<-1

# add this subtaxa number to part of the metacoder tree object
rf_important_metacoder$data$RF_importance_nodes$subtaxa_number<-subtaxa_number

# divide the summed meam importance y the number of subtaxa. ASV nodes shuold ahve the same importance as the original object
rf_important_metacoder$data$RF_importance_nodes$mean_meamImp_per_taxa<-
  rf_important_metacoder$data$RF_importance_nodes$meanImp /
  rf_important_metacoder$data$RF_importance_nodes$subtaxa_number

#Override k__bacteria summed mean importance as it is meaningless
rf_important_metacoder$data$RF_importance_nodes$meanImp[1]<-min(rf_important_metacoder$data$RF_importance_nodes$meanImp)

#define the minimum and maximum ASV importance to plot
min_RF_importance<-min(rf_importance_byOTU$meanImp) 
max_RF_importance<-max(rf_importance_byOTU$meanImp) 

########### done! ######

#plot heat tree
set.seed(1)
output<-heat_tree(rf_important_metacoder,
                 node_size = rf_important_metacoder$data$RF_importance_nodes$mean_meamImp_per_taxa  , # n_obs is a function that calculates, in this case, the number of OTUs per taxon
                 node_label = taxon_names,
                 node_color = rf_important_metacoder$data$RF_importance_nodes$meanImp, # A column from `obj$data$diff_table`
                node_size_interval = c(min_RF_importance, max_RF_importance), # define node size range according RF importances
                 node_label_size_range = c(0.005, 0.015),
                 node_size_range = c(0.009, 0.03),
                 node_size_axis_label = "Size: mean taxon importance",
                 node_color_axis_label = "Color: summed taxon importance",
                 layout = "davidson-harel", # The primary layout algorithm
                 initial_layout = "reingold-tilford") # The layout algorithm that initializes node locations

return(output)

}

#execute custom function, generating 4 heat trees as phyloseq objects
imporatance_Heat_tree_l<-mapply(function(x,y)
  ps_and_boruta_to_heat_tree(ps_obj = x,boruta_ASV_stats = y),
  x = CSS_BacFunBulkRhizo_ps_l, 
  y=ASV_stats_confirmed,
  SIMPLIFY = FALSE)


#make a pannel of ehat trees
pannel_heat_trees_imp_ASVs<-
  ggarrange(imporatance_Heat_tree_l$Bac_Bulk,
            imporatance_Heat_tree_l$Bac_Rhizo,
            imporatance_Heat_tree_l$Fun_Bulk,
            imporatance_Heat_tree_l$Fun_Rhizo,
            ncol =2, 
            nrow = 2,
            labels = c("Bacteria, bulk",
                       "Bacteria, rhizo",
                       "Fungi, bulk",
                       "Fungi, rhizo"))
#save trees
ggsave(pannel_heat_trees_imp_ASVs,
       filename = "./Results/RF_importance_HeatTree.pdf",
       width = 30,
       height = 30,
       units = "cm")
```


